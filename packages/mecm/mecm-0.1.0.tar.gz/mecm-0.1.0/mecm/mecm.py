# -*- coding: utf-8 -*-
# Author: Quentin Baghi 2017
# ==============================================================================
# This routine perform the Modified expectation maximization algorithm (M-ECM)
# based on missing data imputation by conditional distribution using FFT.
# ==============================================================================
import numpy as np
import cvxopt
from cvxopt import matrix, spmatrix, amd, cholmod, umfpack
from scipy import sparse
from scipy import stats
from scipy import interpolate
from numpy import linalg as LA
import pyfftw
from pyfftw.interfaces.numpy_fft import fft, ifft
pyfftw.interfaces.cache.enable()
import time
import sys
import random
import multiprocessing as mp
#from numba import jit
import copy

from .matrixalgebra import * #covLinearOp,precondLinearOp,matPrecondBiCGSTAB,precondBiCGSTAB,matVectProd,printPCGstatus
from .noise import generateNoiseFromDSP, symmetrize
from .leastsquares import pmesureWeighted,pmesure_optimized_TF
from .localestimator import nextpow2,PSD_estimate



def maxlike(y,M,A,N_it_max=15,eps=1e-4,p=20,Nd=10,N_est=1000,Nit_cg=200,
tol_cg=1e-5,compute_cov = True,verbose = True,PCGalgo = 'scipy'):
    """
    Function estimating the regression parameters for a problem of
    multivariate Gaussian maximum likelihood with missing data,
    using the M-ECM algorithm.

    Parameters
    ----------
    y : numpy array (size N)
        masked data vector
    M : numpy array (size N)
        mask vector (with entries equal to 0 or 1)
    A : numpy array (size N x K)
        design matrix (contains partial derivatives of signal wrt parameters)
    N_it_max : scalar integer
        number of iterations of the M-ECM algorithm
    eps : scalar float
        tolerance criterium to stop the algorithm (default is 1e-8)
    p : scalar integer
        number of lags to calculate the tapered approximation of the
        autocoariance function. This is needed to pre-conditionate the conjugate
        gradients.
    Nd : scalar integer
        number of Monte-Carlo draws to estimate the conditional expectation of
        the noise periodogram with respect to the observed data
    N_est : scalar integer
        number of frequency points where to estimate the noise power spectral
        density (on a logarithmic grid)
    N_it_cg : scalar integer
        maximum number of iterations for the conjugate gradient algorithm.
    tol_cg : scalar float
        tolerance criterium to stop the PCG algorithm (default is 1e-7)
    verbose : boolean
        if True, a message is printed at the end of each iteration, displaying
        the value of the convergence criterion
    compute_cov : boolean
        if True, the covariance of the estimator is computed
    PCGalgo : string {'mine','scipy','scipy.bicgstab','scipy.bicg','scipy.cg','scipy.cgs'}
        Type of preconditioned conjugate gradient (PCG) algorithm to use among

    Returns
    -------
    beta_new : array_like (size K)
        final value of the estimated parameter vector
    cov : None or array_like (size K x K)
        estimated covariance matrix of the parameter vector. If compute_cov is
        False, then cov is None
    beta : array_like (size N_iterations x K)
        vector storing the updates of the parameter vector at each iteration
    y_rec : array_like (size N)
        reconstructed data vector, i.e. conditional expectation of the data
        given the available observations.
    I_condMean : array_like (size N)
        conditional expectation of the noise periodogram
    PSD : PSD_estimate class instance
        class containing all the information regarding the estimated noise PSD


    References
    ----------

    .. [1] Q. Baghi et al, "Gaussian regression and power spectral density estimation with missing data: The MICROSCOPE space mission as a case study," Physical Review D, vol. 93, num. 12, 2016


    Notes
    -----

    """

    # First guess: ordinary least squares
    beta0 = pmesureWeighted(y*M, A, M)
    beta0_norm = LA.norm(beta0)

    # Initialization of variables
    beta_old = np.zeros( np.shape(beta0) )
    beta_vect = copy.deepcopy(beta0)
    beta_new = copy.deepcopy(beta0)



    # Difference from one update to the other
    diff = 1.

    # Number of data points
    N = len(y)
    # Number of Fourier frequencies used for the spectrum calculation
    if N % 2 == 0:
        P = np.int(2*N)
    else:
        P = np.int(2**nextpow2(2*N))

    # Number of positive Fourier frequencies
    n = np.int( (N-1)/2. )

    # Extended mask to the circulant process
    PSD = PSD_estimate(N_est,N,P,h_min = None,h_max = None)
    counter = 0


    # PSD initialization
    PSD.estimate(M*(y-np.dot(A,beta0)))
    S_N = PSD.calculate(N)
    S_2N = PSD.calculate(P)

    diff = 1.0

    if 'scipy' in PCGalgo:
        print("The chosen preconditioned conjugate gradient algorithm is from scipy.")


    while ( ( diff > eps) & (counter <= N_it_max) ) :

        # The new beta becomes the old one
        beta_old = beta_new

        # E step : missing data reconstruction
        # ------------------------------------
        R = np.real(ifft(S_2N))
        y_rec,solver = conjGradImputation(y,A,beta_old,M,S_2N,R,p,Nit_cg,PCGalgo,tol=tol_cg)

        # Conditional draws
        condDraws = conditionalMonteCarlo(M*(y - np.dot(A,beta_old)),solver,M,
        Nd,S_2N,Nit_cg,PCGalgo,tol=tol_cg)
        # Periodogram of the draws
        I_mat = periodogram(condDraws,condDraws.shape[1])
        # Conditional average
        I_condMean = np.mean(I_mat,axis=0)
        #logI_condMean = np.mean( np.log(I_mat,axis=0) )


        # CE1 step : estimation of deterministic model parameters
        # -------------------------------------------------------
        beta_new = np.real(pmesure_optimized_TF(y_rec,A,S_N))
        beta_vect = np.vstack( (beta_vect,beta_new) )

        # CE2 step : estimation of noise model parameters (PSD)
        # -------------------------------------------------------
        PSD.MLestimateFromI(I_condMean,Niter = 1)
        S_N = PSD.calculate(N)
        S_2N = PSD.calculate(P)


        # Update counter
        counter += 1
        # Calculate the relative difference between new update and old
        # parameter estimate
        diff = LA.norm(beta_new-beta_old)/beta0_norm

        if verbose:
            print('--------------------------')
            print('Iteration ' + str(counter) + ' completed, de/e =' + str(diff))
            print('--------------------------')

    print("------ MECM iterations completed. ------")
    if diff > eps:
        raise Warning("Attention: MECM algorithm ended \
        without reaching the specified convergence criterium.")
        print("Current criterium: " +str(diff) + " > " + str(eps))

    print("Computation of the covariance...")
    if compute_cov:
        cov,U = mecmcovariance(A,M,S_2N,solver,PCGalgo,Nit=Nit_cg,tol=tol_cg)
    else:
        cov = None


    return beta_new,cov,beta_vect,y_rec[0:N],I_condMean,PSD



# ==============================================================================
def mecmcovariance(A,M,S_2N,solve,PCGalgo,Nit=150,tol=1e-7,r=1e-15):
    """
    Function estimating the covariance of regression parameters for a problem of
    multivariate Gaussian maximum likelihood with missing data.

    Parameters
    ----------
    A : numpy array (size N x K)
        design matrix (contains partial derivatives of signal wrt parameters)
    M : numpy array (size N)
        mask vector (with entries equal to 0 or 1)
    S_2N : numpy array (size P >= 2N)
        PSD vector
    solve : sparse.linalg.factorized instance
        linear operator which calculates x = C_OO^{-1} b for any vector b
    Nit : scalar integer, optional
        maximum number of iterations for the conjugate gradient algorithm.
    tol : scalar float, optional
        tolerance criterium to stop the PCG algorithm (default is 1e-7)
    r : scalar float, optional (default is 1e-15)
        Cutoff for small singular values when computing the inverse matrix.
        Singular values smaller (in modulus) than rcond * largest_singular_value
        (in modulus) are set to zero.

    Returns
    -------
    cov : array_like (size K x K)
        estimated covariance matrix of the parameter vector

    """

    print("The chosen BICGSTAB algorithm is scipy code.")

    A_obs =  A[M==1,:]
    (N_o,K) = np.shape(A_obs)
    U = np.zeros((N_o,K),dtype=np.float64)
    # Missing data indices
    ind_mis = np.where( M==0 )[0]
    ind_obs = np.where( M!=0 )[0]
    N = len(M)

    # Construct the operator calculating C_OO*x precondBiCGSTAB(x0,b,A_func,Nit,stp,P)
    x0 = np.zeros(N_o)

    if PCGalgo == 'mine':
        Coo_func = lambda x: matVectProd(x,ind_obs,ind_obs,M,S_2N)
        U = matPrecondBiCGSTAB(x0,A_obs,Coo_func,Nit,tol,solve,PCGalgo=PCGalgo)
    elif 'scipy' in PCGalgo:
        Coo_op = covLinearOp(ind_obs,ind_obs,M,S_2N)
        P_op = precondLinearOp(solve,len(ind_obs),len(ind_obs))
        U = matPrecondBiCGSTAB(x0,A_obs,Coo_op,Nit,tol,P_op,PCGalgo=PCGalgo)
        #innerPrecondBiCGSTAB(U,x0,A_obs,Coo_op,Nit,tol,P_op,PCGalgo)


    try:
        cov = LA.pinv( np.dot( np.conj(A_obs.T), U ) ,rcond=r)
    except:
        cov = []
        "SVD did not converge"


    return cov,U

# ==============================================================================
def periodogram(x_mat,nfft,wind = 'hanning'):
    """
    Function computing the periodogram of the input data with a specified number
    of Fourier frequencies nfft (possible zero-padding) and apodization window.

    Parameters
    ----------
    x_mat : numpy array (size Nd x N)
        data for which the periodogram must be computed. Can be a one-dimentional
        vector or a matrix containing a time series in each row. The function
        computes as many periodograms as there are rows in x_mat, and returns
        a matrix of same size as x_mat.

    Returns
    -------
    Per : numpy array (size Nd x N)
        matrix (or vector) of periodogram

    """

    l = len(np.shape(x_mat))

    if l == 1 :
        N = len(x_mat)
        # Windowing
        if wind == 'hanning':
            w = np.hanning(N)
        elif wind == 'ones':
            w = np.ones(N)
        elif isinstance(wind, (list, tuple, np.ndarray)):
            w = wind
        else:
            raise TypeError("Window argument is wrong")

        K2 = np.sum(w**2)
        Per = np.abs( fft( x_mat*w, nfft ) )**2 / K2

    elif l==2 :
        (Nd,N) = np.shape(x_mat)
        # Windowing
        if wind == 'hanning':
            w = np.hanning(N)
        elif wind == 'ones':
            w = np.ones(N)
        elif isinstance(wind, (list, tuple, np.ndarray)):
            w = wind
        else:
            raise TypeError("Window argument is wrong")

        K2 = np.sum(w**2)
        Per = np.abs( fft( np.multiply(x_mat,w), n = nfft, axis = 1 ) )**2 / K2

    return Per


# ==============================================================================
def conditionalDraw(Np,Nit,S_2N,solver,z_o,mu_m_given_o,ind_obs,
ind_mis,M,PCGalgo,tol=1e-7):
    """
    Function performing random draws of the complete data noise vector
    conditionnaly on the observed data.

    Parameters
    ----------
    Np : scalar integer
        Size of the initial random vector which is drawn. The initial vector
        follows a stationary distribution whose covariance matrix is circulant.
        It is then truncated to N_final to obtain the desired distribution whose
        covariance matrix is Toeplitz.
    Nit : scalar integer
        Maximum number of conjugate gradient iterations.
    S_2N : numpy array (size P >= 2N)
        PSD vector
    Coo_func : function
        Linear operator that calculates the matrix-to-vector product Coo x for
        any vector x of size No (number of observed data points)
    solver : sparse.linalg.factorized instance
        linear operator which calculates x = C_OO^{-1} b for any vector b
    z_o : array_like (size No)
        vector of observed residuals
    mu_m_given_o : array_like (size Nm)
        vector of conditional expectation of the missing data given the observed
        data
    ind_obs : array_like (size No)
        vector of chronological indices of the observed data points in the
        complete data vector
    ind_mis : array_like (size No)
        vector of chronological indices of the missing data points in the
        complete data vector
    M : numpy array (size N)
        mask vector (with entries equal to 0 or 1)
    tol : scalar float
        Stoping criteria for the conjugate gradient algorithm, optional

    Returns
    -------
    eps : numpy array (size N)
        realization of the vector of conditional noise given the observed data

    """

    # New implementation: the size of the vector that is randomly drawn is
    # equal to the size of the mask. If one wants a larger vector, one must
    # have to zero-pad the mask: M = np.concatenate((M,np.zeros(Npad)))
    N = len(M)
    DSP = np.sqrt(S_2N*2.)
    v,_,_ = generateNoiseFromDSP(DSP,1.)
    #v = np.array([random.gauss(0,1.) for _ in range(Np)])
    #e = np.sqrt(Np/np.float(N_final))*np.sqrt(Np)*np.real( ifft( np.sqrt(S_2N) * v )[0:N_final] )
    e = np.real(v[0:N]*np.sqrt(np.var(v)/np.var(v[0:N])))

    #u0 = solve(e[self.ind_obs])

    N_o = len(ind_obs)
    u0 = np.zeros(N_o)
    stop = np.std(e[ind_obs])
    #stop = 0

    u = PCGsolve(ind_obs,M,S_2N,e[ind_obs],u0,tol,Nit,solver,PCGalgo)

    #u,sr = conjugateGradientSolvePrecond(u0,e[ind_obs],Coo_func,Nit,stop,solver)

    u_m_o = matVectProd(u,ind_obs,ind_mis,M,S_2N)

    eps = np.zeros(N)

    # Observed data points
    eps[ind_obs] = z_o
    # Missing data points and extension to 2N points
    eps[ind_mis] = mu_m_given_o + e[ind_mis] - u_m_o

    return eps

# ==============================================================================
def conditionalMonteCarlo(eps,solve,M,Nd,S_2N,Nit,PCGalgo,seed = None,tol=1e-7):
    """
    Function performing random draws of the complete data noise vector
    conditionnaly on the observed data.

    Parameters
    ----------
    eps : numpy array (size N)
        input vector of residuals (difference between data and model)
    solve : sparse.linalg.factorized instance
        linear operator which calculates x = C_OO^{-1} b for any vector b
    M : numpy array (size N)
        mask vector (with entries equal to 0 or 1)
    S_2N : numpy array (size P >= 2N)
        PSD vector
    Nit : scalar integer
        Maximum number of conjugate gradient iterations.
    seed : character string, optional
        Regenerate the seed with system time (if None) or chosen integer.
    tol : scalar float
        Stoping criteria for the conjugate gradient algorithm, optional


    Returns
    -------
    condDraws : numpy array (size Nd x N)
        Matrix whose rows are realization of the conditional noise vector

    """

    # Missing data indices
    ind_mis = np.where( M==0 )[0]
    ind_obs = np.where( M!=0 )[0]
    N = len(M)
    Np = len(S_2N)

    # Calculate the residuals at the observed data points
    eps_o = eps[ind_obs]

    # Calculate the conditional mean
    N_o = len(ind_obs)
    x0 = np.zeros(N_o)
    u = PCGsolve(ind_obs,M,S_2N,eps_o,x0,tol,Nit,solve,PCGalgo)

    #u,sr = conjugateGradientSolvePrecond(np.zeros(len(ind_obs)),eps_o,Coo_func,Nit,np.std(eps_o),solve)
    mu_m_given_o = matVectProd(u,ind_obs,ind_mis,M,S_2N)
    # Calculate the stopping criteria
    stop = np.std(eps_o)

    condDraws = np.zeros((Nd,N))
    # Regenerate the seed with system time or chosen integer
    random.seed(seed)

    # ==========================================================================
    # Without parallelization:
    # ==========================================================================
    for i in range(Nd):
        condDraws[i,:] = conditionalDraw(Np,Nit,S_2N,solve,eps_o,mu_m_given_o,
        ind_obs,ind_mis,M,PCGalgo,tol=tol)

    return condDraws





# ==============================================================================
def buildSparseCov2(R,p,N,form=None,taper = 'Wendland2'):
    """
    This function constructs a sparse matrix which is a tappered, approximate
    version of the covariance matrix of a stationary process of autocovariance
    function R and size N x N.

    Parameters
    ----------
    R : numpy array
        input autocovariance functions at each lag (size N)
    p : scalar integer
        number of lags to calculate the tapered approximation of the
        autocoariance function.
    N : scalar integer
        Size of the complete data vector
    form : character string (or None), optional
        storage format of the sparse matrix (default is None)
    taper : string
        type of taper function to smoothly decrease the tapered covariance
        function down to zero.


    Returns
    -------
    C_tap : scipy.sparse matrix
        Tappered covariance matrix (size N x N) with p non-zero diagonals.


    """
    k = np.array([])
    values = list()

    tap = taperCovariance(np.arange(0,N),p,taper = taper)
    R_tap = R[0:N]*tap

    # calculate the rows and columns indices of the non-zero values
    # Do it diagonal per diagonal
    for i in range(p+1):

        Rf = np.ones(N-i)*R_tap[i]
        values.append(Rf)

        k = np.hstack((k,i))



        # Symmetric values
        if i!=0 :
            values.append(Rf)
            k = np.hstack((k,-i))


    return sparse.diags(values, k.astype(int), format=form)

# ==============================================================================
def conjGradImputation(s,A,beta,M,S_2N,R,p,Nit,PCGalgo,tol=1e-7):
    """
    Function performing universal kriging with the model y = A beta + n
    using the conjugate gradient algorithm to solve for C_OO x = b.
    But this imputation function can be generalized to any solver algorithm.
    See "imputation" function for a more general formulation

    Parameters
    ----------
    s : array_like (size N)
        Input signal array
    A : numpy array (size N x K)
        design matrix (contains partial derivatives of signal wrt parameters)
    beta : array_like (size K)
        regression parameter vector
    M : numpy array (size N)
        mask vector (with entries equal to 0 or 1)
    S_2N : numpy array (size P)
        spectrum array: value of the unilateral PSD times fs
    R : numpy array (size P)
        tappered autocovariance array: value of the autocovariance at all lags
    p : scalar integer
        number of lags to calculate the tapered approximation of the
        autocoariance function. This is needed to pre-conditionate the conjugate
        gradients.
    Nit : scalar integer
        maximum number of iterations for the conjugate gradient algorithm
    tol : scalar float
        tolerance criterium to stop the PCG algorithm

    Returns
    -------
    z : float or ndarray
        conditional expectation of the data vector
    solve : sparse.linalg.factorized instance
        linear operator which calculates x = C_OO^{-1} b for any vector b
    """



    N = len(s)
    # Preconditionning : use sparse matrix
    # ======================================================================
    ind_mis = np.where( M==0 )[0]#.astype(int)
    ind_obs = np.where( M!=0 )[0]#.astype(int)


    # Deterministic model for observed and missing data
    # ======================================================================
    A_obs =  A[M==1,:]
    A_mis =  A[M==0,:]
    beta_est = beta.T

    # Preconditionner
    # ======================================================================
    solve = computePrecond(R,M,p=p,taper = 'Wendland2')

    # Residuals of observed data - model
    eps = s[ind_obs]-np.dot(A_obs,beta_est)

    # Conjugate gradients
    # ======================================================================
    # Solve the linear system C_oo x = eps
    #u,sr = conjugateGradientSolvePrecond(np.zeros(len(ind_obs)),eps,Coo_func,Nit,np.std(eps),solve)
    N_o = len(ind_obs)
    x0 = np.zeros(N_o)
    u = PCGsolve(ind_obs,M,S_2N,eps,x0,tol,Nit,solve,PCGalgo)

    # Calculate the conditional expectation of missing data
    x = np.dot(A_mis,beta) +  matVectProd(u,ind_obs,ind_mis,M,S_2N)

    z = np.zeros(N)
    z[ind_obs] = s[ind_obs]
    z[ind_mis] = x


    return z,solve


# ==============================================================================
def taperCovariance(h,theta,taper = 'Wendland1',tau=10):
    """
    Function calculating a taper covariance that soothly goes to zero when h
    goes to theta. This taper is to be mutiplied by the estimated covariance R
    of the process, to discard correlations larger than theta.

    Ref : Reinhard FURRER, Marc G. GENTON, and Douglas NYCHKA,
    Covariance Tapering for Interpolation of Large Spatial Datasets,
    Journal of Computational and Graphical Statistics, Volume 15, Number 3,
    Pages 502–523,2006

    Parameters
    ----------
    h : numpy array of size N
        lag
    theta : scalar float
        taper parameter
    taper : string {'Wendland1','Wendland2','Spherical'}
        name of the taper function


    Returns
    -------
    C_tap : numpy array
        the taper covariance function values (vector of size N)
    """

    ii = np.where(h<=theta)[0]

    if taper == 'Wendland1' :

        C = np.zeros(len(h))

        C[ii] = (1.-h[ii]/np.float(theta))**4 * (1 + 4.*h[ii]/np.float(theta))

    elif taper == 'Wendland2' :

        C = np.zeros(len(h))

        C[ii] = (1-h[ii]/np.float(theta))**6 * (1 + 6.*h[ii]/theta + \
        35*h[ii]**2/(3.*theta**2))

    elif taper == 'Spherical' :

        C = np.zeros(len(h))

        C[ii] = (1-h[ii]/np.float(theta))**2 * (1 + h[ii]/(2*np.float(theta)) )

    elif taper == 'Hanning':
        C = np.zeros(len(h))
        C[ii] = (np.hanning(2*theta)[theta:2*theta])**2


    elif taper == 'Gaussian' :

        C = np.zeros(len(h))
        sigma = theta/5.
        C[ii] = np.sqrt( 1./(2*np.pi*sigma**2) ) * np.exp( - h[ii]**2/(2*sigma**2) )

    elif taper == 'rectSmooth':

        C = np.zeros(len(h))
        #tau = np.int(theta/3)


        C[h<=theta-tau] = 1.
        jj = np.where( (h>=theta-tau) & (h<=theta) )[0]
        C[jj] = 0.5*(1 + np.cos( np.pi*(h[jj]-theta+tau)/np.float(tau) ) )

    elif taper == 'modifiedWendland2':


        C = np.zeros(len(h))

        C[h<=theta-tau] = 1.
        jj = np.where( (h>=theta-tau) & (h<=theta) )[0]

        C[jj] = (1-(h[jj]-theta+tau)/np.float(tau))**6 * (1 + \
        6.*(h[jj]-theta+tau)/tau + 35*(h[jj]-theta+tau)**2/(3.*tau**2))



    return C


# ==============================================================================
def computePrecond(R,M,p=10,ptype = 'sparse',taper = 'Wendland2'):
    """
    For a given mask and a given PSD function, the function computes the linear
    operator x = C_OO^{-1} b for any vector b, where C_OO is the covariance matrix
    of the observed data (at points where M==1).


    Parameters
    ----------
    R : numpy array
        input autocovariance functions at each lag (size N)
    M : numpy array
        mask vector
    p : scalar integer
        number of lags to calculate the tapered approximation of the
        autocoariance function. This is needed to pre-conditionate the conjugate
        gradients.
    ptype : string {'sparse','circulant'}
        specifies the type of preconditioner matrix (sparse approximation of the
        covariance or circulant approximation of the covariance)
    taper : string {'Wendland1','Wendland2','Spherical'}
        name of the taper function. This argument is only used if ptype = 'sparse'

    Returns
    -------
    solve : sparse.linalg.factorized instance
        preconditionner operator, calculating P x for all vectors x

    """


    N = len(M)

    # ======================================================================
    ind_mis = np.where( M==0 )[0]#.astype(int)
    ind_obs = np.where( M!=0 )[0]#.astype(int)
    # Calculate the covariance matrix of the complete data

    if ptype == 'sparse':
        # Preconditionning : use sparse matrix
        C = buildSparseCov2(R,p,N,form="csc",taper = taper)
        # Calculate the covariance matrix of the observed data
        C_temp = C[:,ind_obs]
        #C_OO = C_temp[ind_obs,:]
        #del C_temp
        #del C
        # Preconditionner
        #solve =  sparse.linalg.factorized(C_OO)
        solve =  sparse.linalg.factorized(C_temp[ind_obs,:])

    elif ptype == 'circulant':

        S = np.real(fft(R))
        N_fft = len(S)
        solve = lambda v: np.real(ifft(fft(v,N_fft)/S,len(v)))

    return solve
