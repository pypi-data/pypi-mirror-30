from snakemake.workflow import config
from ymp.config import dir2targets, dir2targets2
from ymp.snakemake import workflow


rule bbmap_index:
    message: "Building BBMap Index for {wildcards.rnagene}"
    output: "ref/names/{rnagene}"
    input: lambda wc: config['rrna']['references']['fasta'][wc.rnagene]
    params: build=lambda wc: config['rrna']['genes'].index(wc.rnagene) + 1
    threads: 1
    shell: """
    module load bbmap
    rm ref/genome/{params.build}/summary.txt
    bbmap.sh build={params.build} ref={input}
    touch {output}
    """



rule bbmap_filter:
    message: "Finding {wildcards.rnagene} reads in {wildcards.dir}/{wildcards.sample}"
    output:  "{dir}.{rnagene}/{sample}.{$PAIRS}.fq.gz",
             ihist="{dir}.{rnagene}/{sample}.bb_ihist",
             qhist="{dir}.{rnagene}/{sample}.bb_qhist",
             stats="{dir}.{rnagene}/{sample}.bb_stats",
             props="{dir}.{rnagene}/{sample}.props",
    input:   "{dir}/{sample}.{$PAIRS}.fq.gz",
             "ref/names/{rnagene}"
    log:     "{dir}.{rnagene}/{sample}.log"
    params: build=lambda wc: config['rrna']['genes'].index(wc.rnagene) + 1
    threads: 17
    shell: """
    module load bbmap
    bbmap.sh \
      in={input[0]} in2={input[1]} \
      outm={output[0]} outm2={output[1]} \
      build={params.build} \
      threads={threads} \
      minid=0.7 \
      pigz \
      statsfile={output.stats} machineout \
      ihist={output.ihist} \
      qhist={output.qhist} \
      >{log} 2>&1

    (
        awk '/#Mean/ {{print "insert_size_avg", $2}}
             /#STDev/ {{print "insert_size_sd", $2}}
            ' {output.ihist}
        tail -n1 {output.qhist} | awk '{{print "read_length", $1}}'
    ) > {output.props}

    """


rule emirge:
    message: "Inferring {wildcards.rnagene} rRNA for {wildcards.sample}"
    input: "{dir}.{rnagene}/{sample}.{$PAIRS}.fq.gz",
           props="{dir}.{rnagene}/{sample}.props",
           ref_fasta=lambda wc: config['rrna']['references']['fasta'][wc.rnagene]
    output: dir="{dir}.{rnagene}.emirge/{sample}.em",
            props="{dir}.{rnagene}.emirge/{sample}.props"
    params:
        ref_bowtie=lambda wc: config['rrna']['references']['bowtie'][wc.rnagene],
        iterations=80,
        join_threshold=1.0
    threads: 7
    log: "{dir}.{rnagene}.emirge/{sample}.log"
    run:
        props = read_propfiles(input.props)
        if props['insert_size_avg'] < 1 or props['insert_size_sd'] < 1:
            shell("""
            mkdir {output.dir}
            (
                cat {input.props}
               echo "reads_used 0"
            ) > {output.props}
            """)
        else:
            shell("""
    rm -rf {output.dir}.tmp
    emirge_amplicon.py \
        {output.dir}.tmp -1 {input[0]} -2 <(zcat {input[1]}) \
        -i {props[insert_size_avg]} \
        -s {props[insert_size_sd]} \
        -l {props[read_length]} \
        -n {params.iterations} \
        -j {params.join_threshold} \
        -f {input.ref_fasta} \
        -b {params.ref_bowtie} \
        -a {threads} \
        --phred33 \
        > {log} 2>&1
     rm -rf {output.dir}
     mv {output.dir}.tmp {output.dir}

     (
         cat {input.props}
         tac {log} | sed -n '/^# reads with at least one reported alignment/ \
                             {{ s/^[^:]*: \([0-9]*\) .*/reads_used \\1/p;q;}}'
     ) > {output.props}
     """)

rule emirge_rename_fasta:
    input: em="{dir}.emirge/{sample}.em",
           props="{dir}.emirge/{sample}.props",
    params:
        tlen=1200
    output: "{dir}.emirge/{sample}.fasta"
    log: "{dir}.emimrge/{sample}.rename.log"
    threads: 1
    run:
        props = read_propfiles(input.props)
        shell("""
        if [ -d {input.em}/iter.80 ]; then
        emirge_rename_fasta.py \
            -p 0.000001 \
            -r {wildcards.sample}- \
        {input.em}/iter.80
    else
        echo
    fi | \
    sed '/^>/ y/ /;/' | \
    awk -F 'NormPrior=' '
        /^>/ {{
            size=int($2 * '{props[reads_used]}' * '{props[read_length]}' / '{params.tlen}' * 100 + .5);
            if (size>0) {{
                P=1;
                print $0 ";size=" size
            }} else {{
                P=0
            }}
        }}
        /^[^>]/ {{
            if (P==1) {{
                print $0
            }}
        }}' \
    > {output}
    """)


rule sina_align_classify:
    message: ""
    input: "{dir1}.{rnagene}.{dir2}/{sample}.fasta"
    output: "{dir1}.{rnagene}.{dir2}.sina/{sample}.fasta"
    params:
        ref = lambda wc: config['rrna']['references']['arb'][wc.rnagene],
    log: "{dir1}.{rnagene}.{dir2}.sina/{sample}.log"
    threads: 1
    shell: """
    module load sina
    sina --in "{input}" \
         --out "{output}" \
         --ptdb "{params.ref}" \
         --search \
         --search-db "{params.ref}" \
         --lca-fields tax_slv:tax_gg:tax_rdp \
         --meta-fmt header \
         --line-length 0 \
         --log-file {log} \
         --turn none
    """

rule sina_align_classify2:
    message: ""
    input: "{dir}/{rnagene}.fasta"
    output: "{dir}.sina/{rnagene}.fasta"
    params:
        ref = lambda wc: config['rrna']['references']['arb'][wc.rnagene]
    log: "{dir}.sina/{rnagene}.log"
    threads: 1
    shell: """
    module load sina
    sina --in "{input}" \
         --out "{output}" \
         --ptdb "{params.ref}" \
         --search \
         --search-db "{params.ref}" \
         --lca-fields tax_slv:tax_gg:tax_rdp \
         --meta-fmt header \
         --line-length 0 \
         --log-file {log} \
         --turn none
    """


rule sina_extract_classification:
    input: "{dir}.sina/{sample}.fasta"
    output: "{dir}.sina/{sample}.class.{tax}.csv"
            #FIXME: use re instead of sed, for more power
    shell: """
    sed -n 's/^>\\([^| :]*\\).*lca_tax_{wildcards.tax}=\\(.*;\)] \[.*/\\1,\\2/p' {input} > {output}
    """


rule cluster_vsearch:
    message: "Clustering {wildcards.dir} at {wildcards.fracid}%"
    output: centroids="{dir}.otu{fracid}/centroids.fasta",
            uc="{dir}.otu{fracid}/centroids.uc"
    input: lambda wc: expand("{dir}/{sample}.fasta", dir=wc.dir, sample=dir2targets(wc))
    log: "{dir}.otu{fracid}/vsearch.log"
    threads: 33
    shell: """
    module load vsearch
    cat {input} | \
    vsearch --cluster_size - \
            --centroids {output.centroids} \
            --uc {output.uc} \
            --sizein --sizeout \
            --id 0.{wildcards.fracid} \
            --threads {threads} \
            > {log} 2>&2
    """


rule vsearch_minsize:
    message: "Filtering clusters by size {wildcards.coverage}"
    input: "{clusters}.fasta"
    output: touch("{clusters}.cov{coverage}.fasta")
    shell: """
    module load vsearch
    vsearch --sortbysize {input} --output {output} --minsize {wildcards.coverage}
    """


rule vsearch_map:
    message: "vsearch map {output}"
    input: db="{dir1}.otu{fracid}/centroids.cov{coverage}.fasta",
           fa="{dir1}/{sample}.fasta"
    output: touch("{dir1}.otu{fracid}/{sample}.cov{coverage}.mapping")
    shell: """
    module load vsearch

    if [ -s {input.fa} ]; then
      vsearch --usearch_global {input.fa} \
              --db {input.db} \
              --userout {output} \
              --userfields query+target+id \
              --id 0.{wildcards.fracid}
    else
      touch {output}
    fi
    """


rule map2coverages:
    message:
        "Assembling coverage table {output}"
    output:
        touch("{dir}.otu{fracid}/coverages{coverage}.csv")
    input:
        dir2targets2("{dir}.otu{fracid}/{sample}.cov{coverage}.mapping")
    run:
        from map2otu import MapfileParser
        parser = MapfileParser()
        parser.read(input)
        parser.write(output[0])


rule coverages2otu:
    message:
        "Assembling OTU table {output}"
    output:
        "{dir}.otu{fracid}/otu.{tax}.cov{coverage}.csv"
    input:
        cov = "{dir}.otu{fracid}/coverages{coverage}.csv",
        cls = dir2targets2("{dir}.sina/{sample}.class.{tax}.csv")
    run:
        import fileinput, csv
        with fileinput.input(input.cls) as cls,\
             fileinput.input(input.cov) as cov,\
             open(output[0], "w") as out:
            writer = csv.writer(out)
            clshash = {row[0]:row[1] for row in csv.reader(cls)}
            clshash['centroid'] = "taxonomy_{}".format(wildcards.tax)
            for line in csv.reader(cov):
                line.insert(1,clshash[line[0]])
                print(line)
                writer.writerow(line)


rule barrnap:
    input: "{dir}/contigs.fasta"
    output: "{dir}.rna/{kingdom}.gff"
    message: "Searching for RNA with barrnap (in {wildcards.dir})"
    log: "{dir}.rna/barrnap.{kingdom}.log"
    threads: 8
    shell: """
    module load barrnap
    barrnap \
        --threads {threads} \
        2> {log} \
        > {output} \
        {input} \
        -k {wildcards.kingdom} \

    """

rule barrnap2SSU:
    input: lambda wc: expand("{dir}.rna/{kingdom}.gff", kingdom="arc bac euk".split(), dir=wc.dir),
           contigs="{dir}/contigs.fasta"
    output: "{dir}.rna/SSU.fasta"
    shell: """
    grep "1[68]S" {input} -h | bedtools getfasta -fi {input.contigs} -bed - -fo {output}
    """

rule barrnap2LSU:
    input: lambda wc: expand("{dir}.rna/{kingdom}.gff", kingdom="arc bac euk".split(), dir=wc.dir),
           contigs="{dir}/contigs.fasta"
    output: "{dir}.rna/LSU.fasta"
    shell: """
    grep "2[38]S" {input} -h | bedtools getfasta -fi {input.contigs} -bed - -fo {output}
    """
