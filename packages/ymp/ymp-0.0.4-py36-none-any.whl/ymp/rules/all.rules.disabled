from ymp.snakemake import config
from ymp.config import icfg
from snakemake.io import expand

from ymp.util import R, glob_wildcards, read_propfiles, get_ncbi_root
from config import dir2targets, dir2targets2

import glob
import yaml

# make jobs always list the host on which they ran
shell.prefix("hostname;")

#RAW     = "00_raw_reads"
RAW = "raw"
TRIMMED = "10_trimmed_reads"


localrules: bbmap_index, all_fasta, all_fq


rule split_by_length:
    message: "Splitting {input} by length"
    input: "{dir}/{sample}.{$PAIRS}.fq.gz"
    output: "{dir}.{length}plus/{sample}.{$PAIRS}.fq.gz",
            "{dir}.{length}minus/{sample}.{$PAIRS}.fq.gz"
    threads: 4
    shell:"""
    zcat {input[0]} | paste - - - - | awk '\
      {{\
        "zcat {input[1]} | paste - - - - " |& getline var;\
        split(var,v);\
        if (length($3) > {wildcards.length} || length(v[3]) > {wildcards.length}) {{\
          print var |& "sed s/\\\\\\\\t/\\\\\\\\n/g | gzip -c > {output[0]}"; \
          print $0  |& "sed s/\\\\\\\\t/\\\\\\\\n/g | gzip -c > {output[1]}" \
        }} else {{\
          print var |& "sed s/\\\\\\\\t/\\\\\\\\n/g | gzip -c > {output[2]}"; \
          print $0  |& "sed s/\\\\\\\\t/\\\\\\\\n/g | gzip -c > {output[3]}" \
        }}\
      }}\
    '
    for n in {output}; do
      if [ ! -e $n ]; then
        touch ${{n%.gz}}
        gzip ${{n%.gz}}
      fi
    done
    """

rule pool_by_column:
    message: "{rule}: Combining {input} into {output}"
    output: "{dir}.by_{column,[^.]+}/{colid}.{P}.fq.gz"
    input: lambda wc: expand("{dir}/{sample}.{P}.fq.gz",P=wc.P,dir=wc.dir,\
                             sample=[sample for sample in config['pe_sample'] \
                              if config['pe_sample'][sample][wc.column] == wc.colid])
    run:
        if len(input) == 1:
            os.symlink(os.path.relpath(input[0], os.path.dirname(output[0])),
                       output[0])
        elif len(input) == 0:
            print(wildcards.colid)
            print(wildcards.column)
            print(wildcards.P)
            print(wildcards.dir)
            wc=wildcards
            sample=[sample for sample in config['pe_sample'] \
                    if config['pe_sample'][sample][wc.column] == wc.colid]
            print("xx " + "::".join(sample))
            return False
        else:
            shell("zcat {input} | gzip > {output}")


rule all_pooled:
    message: "Completed all {wildcards.ext} in directory {wildcards.dir}"
    output: "{dir}/all_{ext}"
    input: lambda wc: expand("{dir}/{target}.{ext}", dir=wc.dir, ext=wc.ext, \
                             target=dir2targets(wc))
    shell: """
    touch {output}
    """

rule all_paired:
    output: "{dir}/all_fq.gz"
    input: "{dir}/all_{$PAIRS}.fq.gz"
    shell: "touch {output}"



bbfiles="bincov.csv stats.txt".split()
rule bbmap_coassembly:
    output: "{dir}{by,.by_[^.]*}.{ass,[^./]+}.map/{sample}.bbmap.{$bbfiles}"
    input: fa = "{dir}.{ass}/contigs.fasta",
           fwd = "{dir}{by}/{sample}.{$PAIRS[0]}.fq.gz",
           rev = "{dir}{by}/{sample}.{$PAIRS[1]}.fq.gz"
    params: base = "{dir}{by}.{ass}.map/{sample}"
    threads: 17
    shell: """
    bbmap.sh threads={threads} machineout \
             nodisk ref={input.fa} \
             in={input.fwd} in2={input.rev} \
             statsfile={params.base}.bbmap.stats.txt \
             covstats={params.base}.bbmap.covstats.csv \
             covhist={params.base}.bbmap.covhist.csv \
             bincov={params.base}.bbmap.bincov.csv \

# These files are too large
#             basecov={params.base}.bbmap.basecov.csv.gz
    """

rule combine_bbstats:
    message: "Compiling stats table from bbmap runs"
    output: "{dir}/mapping_stats.csv"
    input: dir2targets2("{dir}/{sample}.bbmap.stats.txt")
    params: samples = lambda wc: dir2targets(wc)
    run:
        R("""
        f <- function(file, sample) {{
            read.csv(file, header=FALSE, sep="=", check.names=FALSE,
                     col.names=c("variable", sample))
        }}
        df <- Reduce(merge, mapply(f, input, params$samples, SIMPLIFY=FALSE))
        tdf <- data.frame(t(df[,-1]))
        colnames(tdf) <- df[,1]
        sample <- rownames(tdf)
        tdf <- sapply(tdf, function(x) as.numeric(sub("%", "", as.character(x))))
        tdf <- data.frame(sample, tdf)
        write.csv(tdf, file=unlist(output), row.names=FALSE)
        """, input=input, params=params, output=output)


rule combine_bincovs:
    message: "Merging coverage bins into table (in {wildcards.dir})"
    output: "{dir}/coverage.csv"
    input: dir2targets2("{dir}/{sample}.bbmap.bincov.csv")
    run:
        from merge import merge
        merge(output[0], input, collect=b"Cov")

rule filter_bincovs:
    message: "Filtering small bins from coverage"
    output: "{dir}/coverage_filtered.csv"
    input: "{dir}/coverage.csv"
    shell: """
    grep -Ev "[^,]*,(1000|[0-9]+[1-9][1-9][1-9])," {input} > {output}
    """

rule msg_canopy_cluster:
    message:
        "mgs-canopy {wildcards.dir}.cags"
    output:
        clust_t="{dir}.cags/clusters{mod}ssv",
        prof_t="{dir}.cags/profiles{mod}ssv",
        noobs="{dir}.cags/excl_insuf_obs{mod}ssv",
        domtop="{dir}.cags/excl_dom_top3{mod}ssv",
        stats="{dir}.cags/progress{mod}txt"
    input:
        "{dir}/coverage{mod}csv"
    log:
        "{dir}.cags/mgs-canopy{mod}log"
    threads: 33
    shell: """
    module load mgs-canopy
    mgs-canopy --input_file_path <(sed -n '2,$ {{y/ /_/;y/,/ /;s/ /_/;s/ /_/;p}}' {input}) \
               --output_clusters_file_path {output.clust_t} \
               --output_cluster_profiles_file {output.prof_t} \
               --cluster_name_prefix CAG \
               --num_threads {threads} \
               --verbosity info \
               --print_time_statistics > {log} \
               --die_on_kill \
               --not_processed_points_file {wildcards.dir}.cags/not_processed.txt \
               --filtered_out_points_min_obs_file {output.noobs} \
               --filtered_out_points_max_dominant_obs_file {output.domtop} \
               --canopy_size_stats_file {output.stats}
    """

rule mgs_canopy_data_to_csv:
    message:
        "mgs-canopy {wildcards.dir}.cags (converting files)"
    input:
        clust_t = "{dir}.cags/clusters{mod}ssv",
        prof_t = "{dir}.cags/profiles{mod}ssv",
        head = "{dir}/coverage{mod}csv"
    output:
        clust = "{dir}.cags/clusters{mod}csv",
        prof = "{dir}.cags/profiles{mod}csv"
    shell: """
    (
        sed -n '1 s/\([^,]*,[^,]*,[^,]*\),.*/CAG,\\1/p' {input.head};
        sed 'y/\t/,/; s/\(.*\)_\([^_]*\)_\([^_]*\)/\\1,\\2,\\3/' {input.clust_t}
    ) > {output.clust}
    (
        sed -n '1 s/[^,]*,[^,]*,[^,]*/CAG/p' {input.head};
        sed 'y/\t/,/' {input.prof_t}
    ) > {output.prof}
    """


cags_complete, = glob_wildcards("{dir,[^/]*}.cags/log.txt")
rule summarize_cag_runs__grep:
    message: "Parsing CAG logfiles"
    output: temp("cag_stats.txt")
    input: expand("{cags_complete}.cags/log.txt", cags_complete=cags_complete)
    shell: """
    grep -P "^[^:]+: ?[0-9]+$" {input} > {output}
    """

rule summarize_cag_runs__R:
    message: "Tabularizing CAG stats"
    output: "cag_stats.csv"
    input: "cag_stats.txt"
    run:
        R("""
    library("reshape2")
    molten <- read.csv("{input}", header=FALSE, sep=":")
    colnames(molten) <- c("Run", "variable", "value")
    df <- dcast(molten, Run~variable)
    write.csv(df, "{output}", row.names=FALSE)
    """)


rule report_test:
    output:
        "reports/test.html"
    input:
        rmd=srcdir("CAGs.Rmd"),
        cags="raw.ecco.trimAQ20.290minus.by_SUBJECT.spc.map.cags/profiles_filtered.csv",
        ssu_otus="raw.ecco.trimAQ20.290minus.by_SUBJECT.SSU.emirge.otu97/otu.slv.cov100.csv"
    run:
        from ymp.util import Rmd
        Rmd(rmd=input.rmd[0],
            out=output,
            cags=os.path.abspath(input.cags[0]),
            ssu_otus=os.path.abspath(input.ssu_otus[0]))
        print(type(input.cags[0]))

rule bbmap_gene_cat:
    message:
        "BBMap mapping to {wildcards.ref} - {wildcards.sample}"
    output:
        stats="{dir}.mh.blast.map/{ref}__{sample}.stats"
    input:
        ref="{dir}.mh.blast/{ref}.fasta.gz",
        pairs="{dir}/{sample}.{: pairnames :}.fq.gz"
    log:
        "{dir}.mh.blast.map/{ref}__{sample}.log"
    threads:
        8
    conda:
        srcdir("bbmap.yml")
    shell: """
    bbmap.sh nodisk pigz unpigz machineout \
      threads={threads} \
      ref={input.ref} \
      in={input.pairs[0]} in2={input.pairs[1]} \
      minid=0.99 \
      statsfile={output.stats} > {log} 2>&1
    """

rule gene_presence_table:
    message:
        "IMP creating RMPM table {output}"
    output:
        stats="{dir}.mh.blast.map/{ref}.csv"
    input:
        "{dir}.mh.blast.map/{ref}__{: runs :}.stats"
    params:
        re="{dir}.mh.blast.map/{ref}__(.+).stats"
    run:
        import csv, fileinput
        sample_re = re.compile(params.re)
        fields = ['sample', 'reads', 'mapped', 'rmpm']
        with open(output[0], "w") as outf, fileinput.input(input) as f:
            out = csv.DictWriter(outf, fields)
            out.writeheader()
            for line in f:
                if f.isfirstline():
                    data={}
                    sample = sample_re.match(f.filename()).group(1)

                if not "=" in line: continue
                key, val = line.strip().split('=')
                try:
                    val = int(val)
                except Exception as e:
                    continue

                data[key] = val

                if key == "R2_Mapped_Reads":
                    row = {
                        'mapped': data['R1_Mapped_Reads'] + data['R2_Mapped_Reads'],
                        'reads':data['Reads_Used'],
                        'sample':sample
                    }
                    row['rmpm'] = row['mapped'] / row['reads'] * 1000000
                    out.writerow(row)

rule gene_presence_plots:
    message:
        "IMO creating RMPM report"
    input:
        csv="{dir}.mh.blast.map/{ref}.csv",
        map="mapping.csv",
        rmd=srcdir("Genes.Rmd")
    output:
        "reports/{dir}.mh.blast.map__{ref}.pdf"
    run:
        from ymp.util import Rmd
        Rmd(rmd=input.rmd[0],
            out=output,
            csv=os.path.abspath(csv),
            map=os.path.abspath(map)
            )

rule my_scan:
    message:
        "Scanning {wildcards.dir} {wildcards.sample}"
    input:
        "{dir}/{sample}.{:pairnames:}.fq.gz"
    output:
        "{dir}.scan/{sample}.bhist.csv"
    threads:
        2
    log:
        "{dir}.scan/{sample}.log"
    params:
        basename="{dir}.scan/{sample}",
        k=7, s=7, o=0, n=8
    shell: """
    ./scan.py -t fq -k {params.k} -s {params.s} -o {params.o} -n {params.n} \
              {input} {params.basename} >& {log}
    """

rule my_scan_mapped:
    message:
        "Scanning BAMs {wildcards.dir} {wildcards.sample}"
    input:
        bam="{dir}.map/{sample}.contigs.{mapper}.sorted.bam",
        bai="{dir}.map/{sample}.contigs.{mapper}.sorted.bam.bai"
    output:
        "{dir}.map.cov/{sample}.contigs.{mapper}.kmerhist_k{k}_s{s}_o{o}_n{n}.csv"
    log:
        "{dir}.map.cov/{sample}.contigs.{mapper}.scan.log"
    params:
        basename="{dir}.map.cov/{sample}.contigs.{mapper}",
        k="{k}",
        s="{s}",
        o="{o}",
        n="{n}"
    shell: """
    ./scan.py -t bam -k {params.k} -s {params.s} -o {params.o} -n {params.n} \
              {input.bam} {params.basename} >& {log}
    """

rule myscan_all:
    message:
        "Completed all scans for {wildcards.dir}"
    input:
        "{dir}.scan/{:runs:}.bhist.csv"
    output:
        "{dir}.scan/all"


rule myscan_allx:
    message:
        "Completed all scans for {wildcards.dir}"
    input:
        "{dir}.scan/{:runs:}.{mapper}.kmerhist_k7_s7_o0_n8.csv"
    output:
        "{dir}.scan/all_{mapper}"
